---
title: "PSTAT 231 Final Broderick-Weinberg"
author: "Hailey Broderick and Callum Weinberg"
date: "March 11, 2022"
output:
  html_notebook:
    code_folding: show
  html_document:
    code_folding: show
bibliography: works_cited.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# This is necessary given the nested directories.
# Enter your working directory here
knitr::opts_knit$set(root.dir = "/home/clw/Documents/UCSB/Quarter6/PSTAT 231/Final Project/PSTAT231_Final")
```


# Executive Summary

# Introduction

This report has been completed by Hailey Broderick and Callum Weinberg for PSTAT 131/231 in the Winter quarter of 2022 at the University of Santa Barbara. For course credit, the report has been submitted to Gauchospace, along with the structured folder which contains the raw data, additional markdown files (the necessary code is available below unless otherwise stated, but these may show some extra work), a codebook/data dictionary, data crosswalks, intermediate data files, a works cited document (called "works_cited.bib"), and the RMd file that creates this HTML file. If accessing this project via online interface, the materials can be found here: https://github.com/leoncw/PSTAT231_Final. The web hosted version of this HTML document can be found HERE (INSERT THIS).

The primary purpose of this project and the course final assignment was to take a dataset and fit models to predict values of a variable. Specifically, the prediction is perfomed on a known variable (i.e. this is a supervised learning project) and a test data set is used to evaluate the performance of the models. Performing inferential statistics on the models was not the primary purpose of this project, but is discussed at times in the report.

The structure of the report is as follows:

* Executive Summary
* Introduction
    + Background
    + Data: Global Environmental Indicators
    + Purpose and Methods
* Coding Requirements
    + Libraries
    + Functions
* Data Cleaning
    + Manual Crosswalks
    + Loading and Combining Datasets
    + Data Limitation
    + Data Interpolation
* Analysis and Prediction
    + Exploratory Data Analysis
    + Training and Test Data
    + Model 1: Regularized Linear Model
    + Model 2: Regularized Generalized Linear Model (Beta Regression)
    + Model 3: Principal Component Analysis and Regression
    + Model 4: Random Forest Model
    + Model 5: Boosted Tree Model
* Results
* Conclusion
    + Summary of Report Results
    + Limitations and Future work

## Background

The United Nations Statistics Division defines renewable energy production as the following: [@Shea2014]

"Renewable electricity production  refers to the proportion of total electricity produced that comes from a renewable origin. Electricity production refers to gross electricity production, which is the sum of the electrical energy production by all the generating units/installations concerned (including pumped storage) measured at the output terminals of the main generators. This includes the consumption by station auxiliaries and any losses in the transformers that are considered integral parts of the station. Renewable electricity production was calculated as the sum of electricity produced from hydro, geothermal, solar, wind, tide, wave and ocean sources. All electricity production from combustible fuels is considered non-renewable; therefore electricity produced from burning biomass or renewable waste is not included as renewable electricity in this table. However, this has been observed to be a relatively negligible proportion of electricity production in most cases." CITE "Renewable elec production percentage.xlsx""

## Data: Global Environmental Indicators



[DISCUSSION OF WHICH FOLDERS, CSVS, and VARIABLES are kept]

[CITE Kaggle and world bank]

## Purpose and Methods

# Coding Requirments

The following libraries are needed to run the code throughout this report. They should be installed on a machine using the "install.packages()" function in R before loading.

```{r libraries, message=FALSE}
library(knitr)
library(plyr)
library(dplyr)
library(tidyr)
library(assertr)
library(ggplot2)
library(reshape2)
library(bamlss)
library(cowplot)
library(glmnet)
```

# Functions

Below are a series of functions (not from libraries) used in different places throughout the report. They are written by the authors unless otherwise specified.

The below function checks for missing values in a data frame or matrix.
```{r function_missing, class.source = 'fold-hide', echo = TRUE, results='hide'}
# From https://cran.r-project.org/web/packages/assertr/vignettes/assertr.html
not.missing.p = function(x){
  if(is.na(x)) return(FALSE)
}
```

The below function replaces missing values in a matrix with the median of the column.
```{r function_median_replacement, class.source = 'fold-hide', echo = TRUE, results='hide'}
# Define Median Replacement function (for a matrix)
# Used for data interpolation
median_replace = function(x) {
  for(i in 1:ncol(x)) {
    # Note: this will break if an entire column is missing values
    x[which(is.na(x[,i])),i] = median(x[,i],na.rm=TRUE)
  }
  return(x)
}
```

The below function creates a quantile-quantile plot using ggplo2 library.
```{r function_ggplot_qq, class.source = 'fold-hide', echo = TRUE, results='hide'}
#Function for QQ Plot in GGPLOT
# Source: https://stackoverflow.com/questions/4357031/qqnorm-and-qqline-in-ggplot2
qqplot_residuals <- function (vec) # argument: vector of numbers
{
  # following four lines from base R's qqline()
  y <- quantile(vec[!is.na(vec)], c(0.25, 0.75))
  x <- qnorm(c(0.25, 0.75))
  slope <- diff(y)/diff(x)
  int <- y[1L] - slope * x[1L]

  d <- data.frame(resids = vec)

  ggplot(d, aes(sample = resids)) + 
    stat_qq(color = "blue", alpha = .2) + 
    geom_abline(slope = slope, intercept = int) + 
    theme(plot.title = element_text(hjust = 0.5)) + 
    theme_minimal()
}
```

# Data Cleaning

## Manual Crosswalks

As mentioned above, within each folder from the Kaggle dataset, the differnt CSV files sometimes had the countries indexed to different values. In other words, there was not a unique identifier between the different CSV files for the Energy and Mineral folder and the Land and Agrigulture folder. In order to combine (merge) these datasets, manual "crosswalks" had to be compiled, with a new identifier variable generated. These crosswalk files are available in the "Crosswalks" folder in the both the submission (note this applies to the submission for PSTAT 231) and the hosted github page (see introduction).

Additionally, there is a "final crosswalk" which maps the different subfolders (after the CSV from each sub folder has been combined) together is loaded.

```{r load_manual_crosswalks, class.source = 'fold-hide', echo = TRUE, results='hide'}
## Energy and Minerals Crosswalk
energy_minerals_crosswalk_manual = read.csv("Crosswalks/Energy_Minerals_Crosswalk.csv")
energy_minerals_crosswalk_mineral_manual = read.csv("Crosswalks/Energy_Minerals_Crosswalk_mineral_dataset.csv")

## Land and Agriculture Crosswalks
land_agriculture_preliminary_manual = read.csv("Crosswalks/land_agricultural_preliminary.csv")
land_agriculture_final_manual = read.csv("Crosswalks/land_agricultural_final.csv")

## Final Crosswalk for Mapping Categories
## Includes World Bank Data
final_crosswalk = read.csv("Crosswalks/02_final_crosswalk.csv")
final_crosswalk = final_crosswalk %>%
  rename(Country = Country_Mapping)
# Get version with only Country and ID
final_crosswalk_Country_Only = final_crosswalk %>%
  select(Country, Country_ID_Final)
```

## Loading and Combing CSVs from Subfolders

The CSVs from each subfolder are loaded and cleaned. Cleaning in this case refers to keeping variables of interest, renaming variables, dropping empty rows, and checking that Country and Country ID are unique. For the folders that do have a unique country identifier for all CSVs in the folder, a crosswalk is generated in R and the CSVs are merged using that. Versions of the merged data are saved to the "intermediate" folder - this was done mainly to create the "final crosswalk" and is not strictly necessary if this code is being rerun.

The world bank data variables are included in the final crosswalk: they were manually entered into the CSV.

Air and Climate Code:

```{r air_and_climate, class.source = 'fold-hide', echo = TRUE, results='hide', message = FALSE}
## CH_4 Emissions
CH4_emissions = read.csv("Raw_Data/Air and Climate/CH4_Emissions.csv")

CH4_emissions = CH4_emissions %>%
  rename(Country_ID = Country.ID,
         CH4_latest_year = X.28, 
         CH4 = CH4.emissions..latest.year, 
         CH4_change_1990 = X..change.since.1990, 
         CH4_per_capita = CH4.emissions..per.capita...latest.year) %>%
  select(Country_ID,Country,CH4_latest_year,CH4,CH4_change_1990,CH4_per_capita) %>%
  slice(2:190) %>%
  assert(not.missing.p, Country_ID)

## CO2 Emissions
CO2_emissions = read.csv("Raw_Data/Air and Climate/CO2_Emissions.csv")

CO2_emissions = CO2_emissions %>%
  rename(Country_ID = Country.ID,
         CO2_latest_year = X.28, 
         CO2 = CO2.emissions..latest.year, 
         CO2_change_1990 = X..change.since.1990, 
         CO2_per_capita = CO2.emissions..per.capita...latest.year) %>%
  select(Country_ID,Country,CO2_latest_year,CO2,CO2_change_1990,CO2_per_capita) %>%
  slice(2:191) %>%
  assert(not.missing.p, Country_ID)

## GHG Emissions
GHG_emissions = read.csv("Raw_Data/Air and Climate/GHG_Emissions.csv")

GHG_emissions = GHG_emissions %>%
  rename(Country_ID = Country.ID,
         GHG_latest_year = X.28, 
         GHG = GHG.total.without.LULUCF..latest.year, 
         GHG_change_1990 = X..change.since.1990, 
         GHG_per_capita = GHG.emissions.per.capita...latest.year) %>%
  select(Country_ID,Country,GHG_latest_year,GHG,GHG_change_1990,GHG_per_capita) %>%
  slice(2:192) %>%
  assert(not.missing.p, Country_ID)

## GHG Emissions
GHG_emissions = read.csv("Raw_Data/Air and Climate/GHG_Emissions.csv")
# GHG_sector_total should be the same as GHG, if not investigate

GHG_emissions = GHG_emissions %>%
  rename(Country_ID = Country.ID,
         GHG_latest_year = X.28, 
         GHG = GHG.total.without.LULUCF..latest.year,
         GHG_change_1990 = X..change.since.1990, 
         GHG_per_capita = GHG.emissions.per.capita...latest.year) %>%
  select(Country_ID,Country,GHG_latest_year,GHG,GHG_change_1990,GHG_per_capita) %>%
  slice(2:192) %>%
  assert(not.missing.p, Country_ID)

## GHG Emissions by Sector
# Add this in
# GHG_emissions_by_sector = read.csv("Data/Air and Climate/GHG_Emissions_by_Sector.csv")
# GHG_sector_total should be the same as GHG, if not investigate

## N2O Emissions
N2O_emissions = read.csv("Raw_Data/Air and Climate/N2O_Emissions.csv")

N2O_emissions = N2O_emissions %>%
  rename(Country_ID = Country.ID,
         N2O_latest_year = X.28, 
         N2O = N2O.emissions..latest.year, 
         N2O_change_1990 = X..change.since.1990, 
         N2O_per_capita = N2O.emissions..per.capita...latest.year) %>%
  select(Country_ID,Country,N2O_latest_year,N2O,N2O_change_1990,N2O_per_capita) %>%
  slice(2:189) %>%
  assert(not.missing.p, Country_ID)

## NOx Emissions
# There are some blank lines in this
# file that need to be skipped
NOx_emissions = read.csv("Raw_Data/Air and Climate/NOx_Emissions.csv")

NOx_emissions = NOx_emissions %>%
  rename(Country_ID = Country.ID,
         NOx_latest_year = X.28, 
         NOx = NOx.emissions..latest.year, 
         NOx_change_1990 = X..change.since.1990, 
         NOx_per_capita = NOx..emissions.per.capita...latest.year) %>%
  select(Country_ID,Country,NOx_latest_year,NOx,NOx_change_1990,NOx_per_capita) %>%
  slice(2:173) %>%
  assert(not.missing.p, Country_ID)

## SO2 Emissions
SO2_emissions = read.csv("Raw_Data/Air and Climate/SO2_emissions.csv")

SO2_emissions = SO2_emissions %>%
  rename(Country_ID = Country.ID,
         SO2_latest_year = X.28, 
         SO2 = SO2.emissions..latest.year, 
         SO2_change_1990 = X..change.since.1990, 
         SO2_per_capita = SO2.emissions.per.capita..latest.year) %>%
  select(Country_ID,Country,SO2_latest_year,SO2,SO2_change_1990,SO2_per_capita) %>%
  slice(2:143) %>%
  assert(not.missing.p, Country_ID)


# Append all of the Datasets
crosswalk_air_climate = rbind.fill(CH4_emissions,
                                   CO2_emissions,
                                   GHG_emissions,
                                   N2O_emissions,
                                   NOx_emissions,
                                   SO2_emissions)

# Create the Crosswalk
crosswalk_air_climate = crosswalk_air_climate %>%
  select(Country_ID, Country) %>%
  distinct()

# Check if there are any Duplicates 
# This would mean that for the air and climate datasets
# there are either repeated countries, IDs, or that
# ID is not unique to country between the datasets
# and Vice versa
dim(crosswalk_air_climate[duplicated(crosswalk_air_climate$Country_ID),])[1] == 0
dim(crosswalk_air_climate[duplicated(crosswalk_air_climate$Country),])[1] == 0

# COmbine the datasets
# Start with the Above Created Crosswalk
# And Merge on Each of the Datasets
combined_air_climate = 
  left_join(crosswalk_air_climate,CH4_emissions, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_air_climate = 
  left_join(combined_air_climate,CO2_emissions, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_air_climate = 
  left_join(combined_air_climate,GHG_emissions, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_air_climate = 
  left_join(combined_air_climate,N2O_emissions, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_air_climate = 
  left_join(combined_air_climate,NOx_emissions, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_air_climate = 
  left_join(combined_air_climate,SO2_emissions, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)

# Export Data
save(combined_air_climate, file="Intermediate_Data/001_combined_air_climate.Rdata")
write.csv(combined_air_climate,"Intermediate_Data/001_combined_air_climate.csv", row.names = FALSE)

# Merge in Final Crosswalk to Get Universal ID
combined_air_climate_clean = 
  left_join(final_crosswalk_Country_Only,combined_air_climate, by = "Country") %>%
  filter(!is.na(Country_ID)) %>%
  select(-Country, -Country_ID) %>%
  filter(Country_ID_Final != 999) #Excluded before merging

# Clean Up Environment
remove(crosswalk_air_climate,combined_air_climate, CH4_emissions, CO2_emissions,
       GHG_emissions,N2O_emissions, NOx_emissions, SO2_emissions)
```

Energy and Minerals Code: 

```{r energy_and_minerals, class.source = 'fold-hide', echo = TRUE, results='hide'}
## Renewable Percentage
# Current problem is to model 2017 renewable energy
# so only keeping 2017 variable in this iteration

# Also there are some empty rows and the end of the
# data set
renewable_percentage = read.csv("Raw_Data/Energy and Minerals/Renewable elec production percentage.csv")

renewable_percentage = renewable_percentage %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area, 
         renewable_percent_2017 = X2017) %>%
  select(Country_ID,renewable_percent_2017) %>%
  slice(1:224) %>%
  assert(not.missing.p, Country_ID) %>%
  left_join(energy_minerals_crosswalk_manual, by = "Country_ID")

## Energy Indicators
# YEAR NOT INDICATED
energy_indicators = read.csv("Raw_Data/Energy and Minerals/Energy Indicators.csv")

energy_indicators = energy_indicators %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area, 
         energy_supply_petajoules = Energy.supply..petajoules.,
         energy_supply_per_capita_gigajoules = Energy.supply..per.capita...gigajoules.per.capita.,
         contribution_renewable_to_electric_production = Conribution.of.renewables.to.electricity.production....) %>%
  select(Country_ID,energy_supply_petajoules,
         energy_supply_per_capita_gigajoules, 
         contribution_renewable_to_electric_production) %>%
  slice(1:217) %>%
  assert(not.missing.p, Country_ID) %>%
  left_join(energy_minerals_crosswalk_manual, by = "Country_ID")

## Energy Intensity
energy_intensity = read.csv("Raw_Data/Energy and Minerals/Energy intensity measured in terms of primary energy and GDP.csv")

energy_intensity = energy_intensity %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area, 
         energy_intensity_2017 = X2017) %>%
  select(Country_ID, energy_intensity_2017) %>%
  assert(not.missing.p, Country_ID) %>%
  left_join(energy_minerals_crosswalk_manual, by = "Country_ID")

## Energy Supply Per Capita
energy_supply_per_capita = read.csv("Raw_Data/Energy and Minerals/Energy supply per capita.csv")

energy_supply_per_capita = energy_supply_per_capita %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area, 
         energy_supply_per_capita_2017 = X2017) %>%
  select(Country_ID,energy_supply_per_capita_2017) %>%
  slice(1:222) %>%
  assert(not.missing.p, Country_ID) %>%
  left_join(energy_minerals_crosswalk_manual, by = "Country_ID")

## Energy Supply
energy_supply = read.csv("Raw_Data/Energy and Minerals/Energy supply.csv")

energy_supply = energy_supply %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area, 
         energy_supply_2017 = X2017) %>%
  select(Country_ID,energy_supply_2017) %>%
  slice(1:222) %>%
  assert(not.missing.p, Country_ID) %>%
  left_join(energy_minerals_crosswalk_manual, by = "Country_ID")

## Contribution of Mining to Value Added
# Only keeping 2017
# Missing ID: will need to fix this
mining_percentage = read.csv("Raw_Data/Energy and Minerals/Contribution of mining to value added.csv")

mining_percentage = mining_percentage %>%
  rename(Country = Country.and.area, 
         mining_value_2017 = X2017) %>%
  select(Country,mining_value_2017) %>%
  assert(not.missing.p, Country) %>%
  left_join(energy_minerals_crosswalk_mineral_manual, 
            by = c("Country" = "Country_Corrected")) %>% # Correct Missing ID
  select(-Country) %>%
  left_join(energy_minerals_crosswalk_manual, by = "Country_ID")

# Append all of the Datasets
crosswalk_energy_minerals = rbind.fill(renewable_percentage,
                                   energy_indicators,
                                   energy_intensity,
                                   energy_supply_per_capita,
                                   energy_supply, 
                                   mining_percentage)

# Create the Crosswalk
crosswalk_energy_minerals = crosswalk_energy_minerals %>%
  select(Country_ID, Country) %>%
  distinct()

#write.csv(crosswalk_energy_minerals,"Crosswalks/Base_Energy_Minerals_Crosswalk.csv", row.names = FALSE)

# Check if there are any Duplicates 
# This would mean that for the energy and minerals datasets
# there are either repeated countries, IDs, or that
# ID is not unique to country between the datasets
# and Vice versa
dim(crosswalk_energy_minerals[duplicated(crosswalk_energy_minerals$Country_ID),])[1] == 0
dim(crosswalk_energy_minerals[duplicated(crosswalk_energy_minerals$Country_Corrected),])[1] == 0

# Combine the CSVs
# Start with the Above Created Crosswalk
# And Merge on Each of the Datasets
combined_energy_minerals = 
  left_join(crosswalk_energy_minerals,renewable_percentage, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_energy_minerals = 
  left_join(combined_energy_minerals,energy_indicators, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_energy_minerals = 
  left_join(combined_energy_minerals,energy_intensity, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_energy_minerals = 
  left_join(combined_energy_minerals,energy_supply_per_capita, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_energy_minerals = 
  left_join(combined_energy_minerals,energy_supply, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_energy_minerals = 
  left_join(combined_energy_minerals,mining_percentage, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)

# Export Data
save(combined_energy_minerals, file="Intermediate_Data/002_combined_energy_minerals.Rdata")
write.csv(combined_energy_minerals,"Intermediate_Data/002_combined_energy_minerals.csv", row.names = FALSE)

# Merge in Final Crosswalk to Get Universal ID
combined_energy_minerals_clean = 
  left_join(final_crosswalk_Country_Only,combined_energy_minerals, by = "Country") %>%
  filter(!is.na(Country_ID)) %>%
  select(-Country, -Country_ID) %>%
  filter(Country_ID_Final != 999) #Excluded before merging

# Clean Up Environment
remove(crosswalk_energy_minerals,combined_energy_minerals,renewable_percentage, energy_indicators,
       energy_intensity,energy_supply_per_capita,energy_supply, mining_percentage,energy_minerals_crosswalk_manual, 
       energy_minerals_crosswalk_mineral_manual)
```


Land and Agriculture Code

```{r land_and_agriculture, class.source = 'fold-hide', echo = TRUE, results='hide'}
## Agricultural Land
agriculture_land = read.csv("Raw_Data/Land and Agriculture/Agricultural Land.csv")

# not including change in cultural land since 1990
# missing a good amount. Also not including
# agricultural land actually irrigated, also 
# missing a lot
agriculture_land = agriculture_land %>%
  rename(Country = Country,
         agricultural_area_2013 = Agricultural.area.in.2013..km2., 
         percent_land_agricultural_2013 = X..of.total.land.area.covered.by.agricultural.area.in.2013, 
         arable_land_2013 = Arable.land.in.2013..km2., 
         permanent_crops_2013 = Permanent.crops.in.2013..km2.,
         permanent_meadows_pastures_2013 = Permanent.meadows.and.pastures.in.2013..km2.) %>%
  select(Country,agricultural_area_2013,percent_land_agricultural_2013,arable_land_2013,
         permanent_crops_2013,permanent_meadows_pastures_2013) %>%
  assert(not.missing.p, Country) %>% # ID missing 
  left_join(land_agriculture_preliminary_manual, by = "Country") %>%
  assert(not.missing.p, Country_ID) %>%
  select(-Country) %>%
  left_join(land_agriculture_final_manual, by = "Country_ID")


## Consumption of Fertilizers Per Unit of 
## Agricultural Land
consumption_fertilizer = read.csv("Raw_Data/Land and Agriculture/Consumption of fertilizers per unit of agricultural land area.csv")

consumption_fertilizer = consumption_fertilizer %>%
  rename(Country = Country,
         nitrogen_consumption = Nitrogen, 
         phosphate_consumption = Phosphate, 
         potash_consumption = Potash) %>%
  select(Country,nitrogen_consumption,phosphate_consumption,potash_consumption) %>%
  assert(not.missing.p, Country) %>% # ID missing 
  left_join(land_agriculture_preliminary_manual, by = "Country") %>%
  assert(not.missing.p, Country_ID) %>%
  select(-Country) %>%
  left_join(land_agriculture_final_manual, by = "Country_ID")

## Terrestrial Protected Areas
# All 2018
terrestrial_protected_areas = read.csv("Raw_Data/Land and Agriculture/Terrestrial protected areas.csv")
  
terrestrial_protected_areas = terrestrial_protected_areas %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area,
         terrestrial_protected_areas = Terrestrial.protected.areas) %>%
  select(Country_ID,Country,terrestrial_protected_areas) %>%
  slice(3:213) %>%
  assert(not.missing.p, Country_ID) %>%
  select(-Country) %>%
  left_join(land_agriculture_final_manual, by = "Country_ID")

# Append all of the Datasets
crosswalk_land_agricultural = rbind.fill(agriculture_land,
                                   consumption_fertilizer,
                                   terrestrial_protected_areas)

# Create the Crosswalk
crosswalk_land_agricultural = crosswalk_land_agricultural %>%
  select(Country_ID, Country) %>%
  distinct()

# Check if there are any Duplicates 
# This would mean that for the energy and minerals datasets
# there are either repeated countries, IDs, or that
# ID is not unique to country between the datasets
# and Vice versa
dim(crosswalk_land_agricultural[duplicated(crosswalk_land_agricultural$Country_ID),])[1] == 0
dim(crosswalk_land_agricultural[duplicated(crosswalk_land_agricultural$Country_Corrected),])[1] == 0

# Combine the CSVs
# Start with the Above Created Crosswalk
# And Merge on Each of the Datasets
combined_land_agricultural = 
  left_join(crosswalk_land_agricultural,agriculture_land, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_land_agricultural = 
  left_join(combined_land_agricultural,consumption_fertilizer, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_land_agricultural = 
  left_join(combined_land_agricultural,terrestrial_protected_areas, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)

# Export Data
save(combined_land_agricultural, file="Intermediate_Data/003_combined_land_agricultural.Rdata")
write.csv(combined_land_agricultural,"Intermediate_Data/003_combined_land_agricultural.csv", row.names = FALSE)

# Merge in Final Crosswalk to Get Universal ID
combined_land_agricultural_clean = 
  left_join(final_crosswalk_Country_Only,combined_land_agricultural, by = "Country") %>%
  filter(!is.na(Country_ID)) %>%
  select(-Country, -Country_ID) %>%
  filter(Country_ID_Final != 999) #Excluded before merging

# Clean Up Environment
remove(crosswalk_land_agricultural,combined_land_agricultural,agriculture_land, 
       consumption_fertilizer,terrestrial_protected_areas, land_agriculture_preliminary_manual,
       land_agriculture_final_manual)
```

Natural Disaster Code

```{r natural_disaster, class.source = 'fold-hide', echo = TRUE, results='hide'}
## climatological disasters
climatological_disasters = read.csv("Raw_Data/Natural Disasters/Climatological disasters.csv")

climatological_disasters = climatological_disasters %>%
  rename(Country_ID = CountryID,
         Country = Countries.or.areas, 
         climate_disaster_occurence_2010_2019 = Occurrence.2010.2019, 
         climate_disaster_deaths_2010_2019 = Total.deaths.2010.2019, 
         climate_disaster_affected_2010_2019 = Persons.affected.2010.2019) %>%
  select(Country_ID,Country,climate_disaster_occurence_2010_2019,
         climate_disaster_deaths_2010_2019,
         climate_disaster_affected_2010_2019) %>%
  assert(not.missing.p, Country_ID)

## geophysical disasters
geophysical_disasters = read.csv("Raw_Data/Natural Disasters/Geophysical disasters.csv")

geophysical_disasters = geophysical_disasters %>%
  rename(Country_ID = CountryID,
         Country = Countries.or.areas, 
         geophysical_disaster_occurence_2010_2019 = Occurrence.2010.2019, 
         geophysical_disaster_deaths_2010_2019 = Total.deaths.2010.2019, 
         geophysical_disaster_affected_2010_2019 = Persons.affected.2010.2019) %>%
  select(Country_ID,Country,geophysical_disaster_occurence_2010_2019,
         geophysical_disaster_deaths_2010_2019,
         geophysical_disaster_affected_2010_2019) %>%
  assert(not.missing.p, Country_ID)


## hydrological disasters
hydrological_disasters = read.csv("Raw_Data/Natural Disasters/Hydrological disasters.csv")

hydrological_disasters = hydrological_disasters %>%
  rename(Country_ID = CountryID,
         Country = Countries.or.areas, 
         hydrological_disaster_occurence_2010_2019 = Occurrence.2010.2019, 
         hydrological_disaster_deaths_2010_2019 = Total.deaths.2010.2019, 
         hydrological_disaster_affected_2010_2019 = Persons.affected.2010.2019) %>%
  select(Country_ID,Country,hydrological_disaster_occurence_2010_2019,
         hydrological_disaster_deaths_2010_2019,
         hydrological_disaster_affected_2010_2019) %>%
  assert(not.missing.p, Country_ID)


## meteorological disasters
meteorological_disasters = read.csv("Raw_Data/Natural Disasters/Meteorological disasters.csv")

meteorological_disasters = meteorological_disasters %>%
  rename(Country_ID = CountryID,
         Country = Countries.or.areas, 
         meteorological_disaster_occurence_2010_2019 = Occurrence.2010.2019, 
         meteorological_disaster_deaths_2010_2019 = Total.deaths.2010.2019, 
         meteorological_disaster_affected_2010_2019 = Persons.affected.2010.2019) %>%
  select(Country_ID,Country,meteorological_disaster_occurence_2010_2019,
         meteorological_disaster_deaths_2010_2019,
         meteorological_disaster_affected_2010_2019) %>%
  assert(not.missing.p, Country_ID)

# Append all of the Datasets
crosswalk_natural_disaster = rbind.fill(climatological_disasters,
                                   geophysical_disasters,
                                   hydrological_disasters,
                                   meteorological_disasters)

# Create the Crosswalk
crosswalk_natural_disaster = crosswalk_natural_disaster %>%
  select(Country_ID, Country) %>%
  distinct()

# Check if there are any Duplicates 
# This would mean that for the energy and minerals datasets
# there are either repeated countries, IDs, or that
# ID is not unique to country between the datasets
# and Vice versa
dim(crosswalk_natural_disaster[duplicated(crosswalk_natural_disaster$Country_ID),])[1] == 0
dim(crosswalk_natural_disaster[duplicated(crosswalk_natural_disaster$Country_Corrected),])[1] == 0

# Combine CSVs
# Start with the Above Created Crosswalk
# And Merge on Each of the Datasets
combined_natural_disaster = 
  left_join(crosswalk_natural_disaster,climatological_disasters, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_natural_disaster = 
  left_join(combined_natural_disaster,geophysical_disasters, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_natural_disaster = 
  left_join(combined_natural_disaster,hydrological_disasters, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_natural_disaster = 
  left_join(combined_natural_disaster,meteorological_disasters, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)

# Export Data
save(combined_natural_disaster, file="Intermediate_Data/004_combined_natural_disaster.Rdata")
write.csv(combined_natural_disaster,"Intermediate_Data/004_combined_natural_disaster.csv", row.names = FALSE)

# Merge in Final Crosswalk to Get Universal ID
combined_natural_disaster_clean = 
  left_join(final_crosswalk_Country_Only,combined_natural_disaster, by = "Country") %>%
  filter(!is.na(Country_ID)) %>%
  select(-Country, -Country_ID) %>%
  filter(Country_ID_Final != 999) #Excluded before merging

# Clean Up Environment
remove(crosswalk_natural_disaster,combined_natural_disaster,climatological_disasters, 
       geophysical_disasters,hydrological_disasters, meteorological_disasters)
```

Other Categories Code:

```{r other_categories, class.source = 'fold-hide', echo = TRUE, results='hide'}
## Biodiversity Data/Marine and Terrestrial Protected areas
# row 1 is world total
protected_areas_marine_terrestrial = read.csv("Raw_Data/Biodiversity/Terrestrial_Marine protected areas.csv")

protected_areas_marine_terrestrial = 
  protected_areas_marine_terrestrial %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area,
         protected_areas_marine_terrestrial_latest_year = latest.year.available, 
         protected_areas_marine_terrestrial = Terrestrial.and.marine.protected.areas....of.total.territorial.area.) %>%
  slice(2:211) %>%
  assert(not.missing.p, Country_ID)

## Marine and Coastal Protected areas
# first row is world total
protected_areas_marine_coastal = read.csv("Raw_Data/Marine and Coastal Areas/Marine protected areas.csv")

protected_areas_marine_coastal = 
  protected_areas_marine_coastal %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area,
         protected_areas_marine_latest_year = latest.year.available, 
         protected_areas_marine = Marine.protected.areas....of.territorial.waters.) %>%
  slice(2:217) %>%
  assert(not.missing.p, Country_ID)

## Forests Data
# deforestaion and fire areas not included due to 
# significant number of missing values
# row 1 is world total
forest_areas = read.csv("Raw_Data/Forests/Forest Area.csv")

forest_areas = 
  forest_areas %>%
  rename(Country_ID = CountryID,
         Country = Country.and.Area,
         forest_area_2015 = Forest.Area..2015..1000.ha., 
         forest_area_2020 = Forest.Area..2020..1000.ha.,
         forest_total_land_area_2020 = Total.Land.Area..2020..1000.ha.) %>%
  select(Country_ID,Country,forest_area_2015,forest_area_2020,forest_total_land_area_2020) %>%
  slice(2:237) %>%
  assert(not.missing.p, Country_ID)


## Governance
governance = read.csv("Raw_Data/Governance/Governance.csv")

governance = governance %>%
  rename(Country_ID = CountryID,
         Country = Country.and.area) %>%
  rename_with(~gsub(".", "_", .x, fixed = TRUE)) %>%
  assert(not.missing.p, Country_ID)

# Append all of the Datasets
crosswalk_other = rbind.fill(protected_areas_marine_terrestrial,
                                   protected_areas_marine_coastal,
                                   forest_areas,
                                   governance)

# Create the Crosswalk
crosswalk_other = crosswalk_other %>%
  select(Country_ID, Country) %>%
  distinct()

# Check if there are any Duplicates 
# This would mean that for the air and climate datasets
# there are either repeated countries, IDs, or that
# ID is not unique to country between the datasets
# and Vice versa
dim(crosswalk_other[duplicated(crosswalk_other$Country_ID),])[1] == 0
dim(crosswalk_other[duplicated(crosswalk_other$Country),])[1] == 0

# Combine CSVs
# Start with the Above Created Crosswalk
# And Merge on Each of the Datasets
combined_other = 
  left_join(crosswalk_other,protected_areas_marine_terrestrial, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_other = 
  left_join(combined_other,protected_areas_marine_coastal, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_other = 
  left_join(combined_other,forest_areas, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)
combined_other = 
  left_join(combined_other,governance, by = "Country_ID") %>%
  select(-Country.y) %>%
  rename(Country = Country.x)

# Export Data
save(combined_other, file="Intermediate_Data/005_combined_other.Rdata")
write.csv(combined_other,"Intermediate_Data/005_combined_other.csv", row.names = FALSE)

# Merge in Final Crosswalk to Get Universal ID
combined_other_clean = 
  left_join(final_crosswalk_Country_Only,combined_other, by = "Country") %>%
  filter(!is.na(Country_ID)) %>%
  select(-Country, -Country_ID) %>%
  filter(Country_ID_Final != 999) #Excluded before merging

# Clean Up Environment
remove(crosswalk_other,combined_other,protected_areas_marine_terrestrial, 
       protected_areas_marine_coastal,forest_areas, governance)
```

Combine all Categories Code:

```{r combined_all, class.source = 'fold-hide', echo = TRUE, results='hide'}

# Create Clea Final Data and World Bank Dataset
full_data = final_crosswalk %>%
  filter(Country_ID_Final != 999) %>%
  filter(Country_Final != "Swaziland") %>%
  filter(Country_Final != "Czechia") %>%
  filter(Country_Final != "St. Helena") %>%
  filter(Country_Final != "St. Pierre-Miquelon") %>%
  select(-Country) %>%
  distinct()

# Check if there are any Duplicates 
# This would mean that for the air and climate datasets
# there are either repeated countries, IDs, or that
# ID is not unique to country between the datasets
# and Vice versa
dim(full_data[duplicated(full_data$Country_ID_Final),])[1] == 0
dim(full_data[duplicated(full_data$Country_Final),])[1] == 0

# Create Full Dataset (Excluding 999 Countries)
full_data = left_join(full_data,combined_air_climate_clean, by = "Country_ID_Final")
full_data = left_join(full_data,combined_energy_minerals_clean, by = "Country_ID_Final")
full_data = left_join(full_data,combined_land_agricultural_clean, by = "Country_ID_Final")
full_data = left_join(full_data,combined_natural_disaster_clean, by = "Country_ID_Final")
full_data = left_join(full_data,combined_other_clean, by = "Country_ID_Final")

# Save the Data for Use in the Next File
save(full_data, file="Intermediate_Data/01_full_data.Rdata")
remove(combined_air_climate_clean,combined_energy_minerals_clean,combined_land_agricultural_clean,
       combined_natural_disaster_clean,combined_other_clean,final_crosswalk,final_crosswalk_Country_Only)
```


## Limit the Data

The following code limits the observations and variables based on a number of practical criteria. First, any missing values are replaced with NAs. Non-numeric data is converted to numeric (except for country names). Variables that were originally included from the CSVs as "Notes" are excluded. Additionally, the renewable percentage for 2017 is converted to a percentage (the data is a percentage from the source, not a decimal). 

[NOTE NUMBER OF ROWS AND COLUMNS DROPPED]

```{r additional_data_cleaning, class.source = 'fold-hide', echo = TRUE, results='hide'}
## Load the Data from the 01 File
load(file="Intermediate_Data/01_full_data.Rdata")

## Rename Data
full_data_clean = full_data
remove(full_data)

# Convert ... to NA
full_data_clean = full_data_clean %>%
  mutate(across(.cols = everything(), ~ifelse(.x == "...", NA, .x)))

# Remove Note Columns
full_data_clean = full_data_clean %>%
  select(-World_Bank_Notes,-Other_Notes)

# Convert Variables to Numeric
# Everything besides Country and Country ID should be numeric
#full_data_clean = full_data_clean %>%
#  mutate(across(.cols = everything(), ~ifelse(is.character(.x) == TRUE, as.numeric(.x), .x)))
j = ncol(full_data_clean)
i = c(2:j)
full_data_clean[ , i] = apply(full_data_clean[ , i], 2, function(x) as.numeric(x))

# Convert Renewable Percentage 2017 to an Actual Percentage
full_data_clean$renewable_percent_2017 = 
  full_data_clean$renewable_percent_2017/100
```

Any observations (countries) that don't have percent renewable for 2017 are excluded, since that is what predicted, and there is no reasonable way to interpolate the "solution." 19 countries do not have 2017 renewable percentage.

```{r limit_missing_renewables, echo = TRUE, results='hide'}
## Drop Observations that don't have Renewable Energy Percentage for 2019
# There are 19 Such observations
full_data_clean = full_data_clean %>%
  filter(is.na(renewable_percent_2017) == FALSE)
```


Some of the variables and countries are missing a lot of values: more than is reasonable to interpolate. The selected threshold is if the country is missing more than 50 values. This was chosen after reviewing the dataset, and weighs the importance of retaining data (and not biasing the model too much by removing countries) with the practical consideration that fitting a model for an observation that is over mostly interpolated for its predictors is not particularly useful or intersting and makes modeling more dificult.

```{r remove_sparse_observations}
## Get a count of the number of missings, and remove Countries 
# that have 50 or more missing values. See write up for list of
# countries
full_data_row_remove = full_data_clean %>%
  mutate(row_missings = rowSums(is.na(full_data_clean))) %>%
  filter(row_missings < 50)
```

After removing the observations (countries) without much data, a similar process is applied to columns (predictors). Similar logic applies as above. Variables that are mostly missing will be mostly imputed, reducing the value of the process. 

Per capita variables are not included when the raw total is available (since models will be able to include population). Year variables are not included in modeling, but are available for data tracking in "01_full_data.Rdata". Variables are removed "manually" in this step by inspecting the list of variables and number of missings

```{r limit_to_variables}
# Below is a list of the number of missing values per column. 
# Using this list to determine what variables to keep for the final models
# Uncomment the below code for a table of the columns 
# kable(as.matrix(colSums(is.na(full_data_row_remove))))

# See write up for more information regarding model selection. 
# Generally, per capita variables are excluded if the raw value
# is available, given the population variable is present

# N2O and SO2 are missing for too many values to be included
# percent_land_agricultural_2013 is included since agricultural_area_2013 
# is missing for most values (ideally the latter would be kept since 
# Square kilometers is already included)

# Variables with signicant numbers of missings are dropped
limited_data = full_data_row_remove %>%
  select(c(Country_Final,Country_ID_Final,renewable_percent_2017,X2017_Population_World_Bank,X2017_Land_Area_km_2,X2017_GDP_US_Dollars_World_Bank,
           CH4,CO2,GHG,NOx,energy_supply_petajoules,contribution_renewable_to_electric_production,energy_intensity_2017,energy_supply_2017,
           mining_value_2017, percent_land_agricultural_2013,nitrogen_consumption,phosphate_consumption,potash_consumption,terrestrial_protected_areas,
           protected_areas_marine_terrestrial,forest_area_2015,Basel_Convention,CITES,Convention_on_Biological_Diversity,Kyoto__Protocol,Montreal_Protocol,
           Paris_Agreement,Ramsar_Convention,Rotterdam_Convention,Stockholm_Convention,UN_Convention_on_the_Law_of_the_Sea,
           UN_Convention_to_Combat_Desertification,UN_Framework_Convention_on_Climate_Change,World__Heritage_Convention))

# Save the Data for Use in the Next File
save(limited_data, file="Intermediate_Data/02_limited_data.Rdata")
# Clean Environment
remove(full_data_clean,full_data_row_remove,i,j)
```

Note that after starting the modeling process, it was discovered that energy_supply_2017 and energy_supply_petajoules contained identical data from two different sources. energy_supply_2017 was not included in the models, but appears in the data cleaning process above.

# Interpolating Missing Data

[Discussion of this section. Reference textbook section that talks about interpolation]

The goal is to predict each missing value using a linear model of all other observations, where any missings in the fit for the model are substituted with the median (but only for the fitting and prediction stage).

Additionally, contribution_renewable_to_electric_production was removed at this step. It was discovered while modeling the variable was basically the same as  rewable_percentage_2017, and likely is a slightly different version of the same information.

```{r data_interpolation, warning = FALSE}
## Load the Data from the 02 File
load(file="Intermediate_Data/02_limited_data.Rdata")

# Highly Correlated, remove from data
# contribution_renewable_to_electric_production
limited_data = limited_data[,-12]

# Count Number of Missings
# Uncomment code to determine number of missings
# colSums(is.na(limited_data))


# Define Model Data
model_data = limited_data

# Loop Over Each Variable Not in 
for(i in 7:ncol(limited_data)) {

  # Get Vector of Predictors (Column Position in limited_data)
  predictors = c(7:34)
  predictors = predictors[predictors != i]

  # Fit the Linear Model on All Precitors (excluding the)
  # variable that is being interpolated
  # Median Replace Function defined in functions section
  fit = 
    lm(data = limited_data, 
       as.matrix(limited_data[,i]) ~ as.matrix(median_replace(limited_data[,predictors])))
  
  # Predict
  prediction = predict(fit,as.data.frame(median_replace(limited_data[,predictors])))
  
  # No Predictions should be less than 0
  # This is based on the variables included, and
  # may change if more variables are included
  negative_indeces = which(prediction < 0)
  prediction[negative_indeces] = 0
  
  # Replace Data With Interpolated Data
  replace_indeces = which(is.na(model_data[,i]))
  model_data[replace_indeces,i] = prediction[replace_indeces]
}

# Save the Data for Use in the Next File
save(model_data, file="Intermediate_Data/03_model_data.Rdata")
# Clean Environment
remove(fit,i,negative_indeces,
       prediction,predictors,replace_indeces)
```

The interpolation process was found to produce values that replaced the missing values in a reasonable manner. All of the variables were checked to determine none of the interpolated values were outliers compared to the emperical distribution of the observed values for each variable. A histogram of "Ramsar Convention Year" is shown below.

```{r}
# Create a Plot to Compare Data Sets before and After
distr_df = data.frame(Filled = model_data$Ramsar_Convention, 
                      Missing = limited_data$Ramsar_Convention) %>% 
  melt(value.name='y_i')

# Plot
ggplot(distr_df, aes(x = y_i, fill = variable)) +
  geom_histogram(position = "identity",binwidth = 2, alpha = .4) +
  labs(x = "y_i", y = "Frequency", 
       title = "Comparison") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 5)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 12))
```

This graph illustrates what occurs after interpolating the missing values in the Ramsar Convention variable. For the missing values, there year is being predicted. The interpolated values are mostly towards the center of the distribution, with some weight towards the higher end (later year) of the distribution.  If a country has not signed the convention, it arguably makes sense they would be interpolated to sign later (since signing later implies less enthusiasm for the convention, as does signing later).

# Exploratory Data Analysis

[Fill In]


# Training and Test Data Set

[Include discussion of training and Test Data Set]

```{r train_test_split}
## Load the Data from the 02 File
load(file="Intermediate_Data/03_model_data.Rdata")
# Set Seed for Reproducibility
set.seed(42)

# training set 80% of original data
train = sample(nrow(model_data), .8*(nrow(model_data)))
x.train = model_data[train, ]
y.train = model_data[train, ]$renewable_percent_2017
# test set remaining 20% of original data
x.test = model_data[-train, ]
y.test = model_data[-train, ]$renewable_percent_2017
```


# Model 1: Regularized Linear Model

[Fill In]


# Model 2: Regularized Beta Regression Model

## Beta Regression Justification

When fitting a linear model for prediction (or interpretation), the "linear model" (often referred to as "ordinary linear regression") is often considered the simplest and first choice for a continuous response. However, such model is not alwyas appropriate. The linear model estimates the response directly as a linear combination of the $\mathbf{\beta}$ matrix and $\mathbf{X}$ matrices, with an error matrix as well. This is of the form:

$$ \mathbf{Y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$$

where $\mathbf{Y}$ and $\mathbf{\epsilon}$ are $n\times 1$ matrix, $\mathbf{X}$ is a $n\times p$ matrix, and $\mathbf{\beta}$ is a $p\times 1$ matrix. The $\beta$ coefficients are fit using the data with least squares estimates, and the residuals can be analyzed for normality. Such a model clearly assumes additive error. Additionally, it is generally desirable for the errors to be normally distributed (and therefore this can be reffered to as a Gaussian model) for inference purposes, although it is not strictly necessary for estimating the coefficients.

Practically, issues with the additive error term become apparent when we consider what constraints we may want on $\mathbf{Y}$. For renewable energy percentages in 2017, we clearly want a model that predicts a percentage, i.e. values between 0 and 1. The simple linear model does not guarentee this: coefficients could be estimated for values less than 0 or greater than 1. Therefore it is appropriate classes of models from the more "general" set of "generalized linear models." The genearlized linear model allows for the $\mathbf{X}\mathbf{\beta}$ matrix (called the systematic component - this portion must be linear in the $\beta's$) to be related to the response via a link function, and incorporates the error into a "random" component. 

The choice of generalized linear model (GLM) depends on what values the response can take on. Logistic regression (technically bernoulli regression) is a form of GLM that applies for binary responses, and uses the logit link function. In this problem, the response is continous. When applying linear models to proportions, it must be determined whether the underlying values are counts or an actual proportion. If they are counts, then binomial regression (bernoulli regression bu $n$ does not have to equal 1) or poisson regression are usually appropriate starting places. In this problem it is a bit unclear whether the values are actually counts divided by a total, or actually proportions. Generally energy is thought of as a continupus value, and not a discrete count. Therefore, the less common beta regression model is likely the most appropriate GLM here [CITE https://rcompanion.org/handbook/J_02.html]. 

Beta regression assumes the response variable follows a beta distribution.^[the probability density function is $f(y;\mu,\phi) = \frac{\Gamma(\phi)}{\Gamma(\mu\phi)\gamma((1-\mu)\phi)}y^{\mu\phi-1}(1-y)^{(1-\mu)\phi-1}$] It also requires the response variable to be between 0 and 1, but not exactly 0 or 1. This is a problem for this dataset, as a few renewable percentages are 0 or 1. In order to make use of the model in spite of this, a small ammount of bias is introduced by setting values from the training set that are 0 equal to 0.000001 and values that are 1 equal to 0.999999. The model for a beta regression is as follows:

$$g(\mu_i) = \mathbf{X\beta}$$

where the mean percentage, $\mu$ accounts for the randomness and is modeled as a function of the systematic component, $\mathbf{X\beta}$. $g(.)$ is the link function. CITE[https://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf]. Different link functions are possible, but the default method for beta regression is typically the logit function, $g(\mu) = log(\frac{\mu}{1-\mu})$. This results in the model 

$$log\bigg(\frac{\mu_i}{1-\mu_i}\bigg) = \mathbf{X\beta}$$

This looks similar logistic (bernoulli) regression. However the beta regression model incorporates a second parameter as well; the precision $\phi$ sometimes referred to as the dispersion $\phi^{-1}$. The precision can be constant or modeled. CITE[https://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf]. In the package used in this project it is fit using a logit link as well and is modeled to be approximately 1, see below. 

There is a computation problem related to fitting this model in R. Fitting this model is generally done using the \textif{betareg} package. However as outlined in the purpose and methods section, this report seeks to fit sparse models using some form of regularization, since there are approximately 30 predictors. The \textif{betareg} does not have a regularization option, and the \textif{glmnet} package does not have a beta distribution option.^[Writing a new package that does this was considered outside the scope of this project] Of existing packages, the \textif{bamlss} was deemed to provide a modeling technique most appropriate given the goals of the project.[CITE https://arxiv.org/pdf/1909.11784.pdf]

Information of the \textif{bamlss} package can be found here CITE[https://arxiv.org/pdf/1909.11784.pdf] and here http://www.bamlss.org/. \textif{bamlss} stands for "Bayesian additive models for location, scale and shape." CITE[https://arxiv.org/pdf/1909.11784.pdf] The package appears very powerful and capable of bayesian generalized additive models. For the purposes of this report, only a generalized linear model is needed. Instead of maximum likelihood estimation to estimate the parameters, a bayesian approach is implemented, with prior distributions assigned to the $\beta's$.CITE[https://arxiv.org/pdf/1909.11784.pdf page 12] The package also automatically implements a lasso-type penalty, inducing regularization and setting some of the coefficients to 0, thus creating a sparse model. CITE[https://arxiv.org/pdf/1909.11784.pdf page 9].

Note that the package uses BIC (Bayesian Information Criterion) to select the optimal model. This is different than the other methods in this report, which rely on cross validation. To the best of the authors's knowledge, a simple ti implement cross-validation approach is not available for the \textif{bamlss} package.


```{r regularized_beta_regression, class.source = 'fold-show', echo = TRUE, results='show'}
## First, set the seed for reproducibly.
set.seed(42)

# Beta Regression Values must be strictly between 0 and 1
y.train = ifelse(y.train == 0,y.train + .000001,y.train)
y.train = ifelse(y.train == 1,y.train - .000001,y.train)

## Model formula.
## y.train instead of renewable_percent 2017
beat_regression_model = y.train ~ X2017_Population_World_Bank + X2017_Land_Area_km_2 + X2017_GDP_US_Dollars_World_Bank + 
  CH4 + CO2 + GHG + NOx + energy_supply_petajoules + energy_intensity_2017 +
  mining_value_2017 + percent_land_agricultural_2013 + nitrogen_consumption + phosphate_consumption +
  potash_consumption + terrestrial_protected_areas + protected_areas_marine_terrestrial + forest_area_2015 + Basel_Convention +
  CITES + Convention_on_Biological_Diversity + Kyoto__Protocol + Montreal_Protocol + Paris_Agreement + Ramsar_Convention + 
  Rotterdam_Convention + Stockholm_Convention + UN_Convention_on_the_Law_of_the_Sea + UN_Convention_to_Combat_Desertification +
  UN_Framework_Convention_on_Climate_Change + World__Heritage_Convention
  # + energy_supply_2017

## Estimate model.
## The data is input in matrix form
beta_model = bamlss(beat_regression_model, family = "beta", 
                    data = cbind(y.train,x.train), 
                    optimizer = lasso, criterion = "BIC", 
                    n.iter = 12000, burnin = 2000, thin = 10)

## Summary of the model
summary(beta_model)
```

The model is fit above. There are two important notes here. First, the energy_supply_2017 variable appears to cause some issue in fitting the model and is excluded. Secondly, since the bamlss function takes a bayesian approach, the $\beta's$ are modeled with posterior distributions. This involves sampling the posterior distribution using mote carlo estimation. The "n.iter", "burnin", and "thin" options relate to the sampling and the selected values were defaults suggested in a Vignette on the bamlss wepage.[https://bayesr.r-forge.r-project.org/articles/glm.html]

The above model summary indicates a few important pieces of information. First, it is staed that $\mu$ and the variance were fit with the logit link function. The variance parameter ended up being 1. Samples from the posterior distribution of each $\beta$ coefficient are available. Note that if they 95\% posterior credible interval for each parameter contains 0, the parameter coefficeint is set to 0. 

The world bank demographic variables, the emissions, and the energy supply are all set to 0 due to regularization. The value for the paris agreement $\beta$ is largest in absolute value. $e^{\beta_{Paris Agreement}} = 1.1096$, which can be interpreted as a one year increase in a country signing the paris agreement results in a 11/% decrease in the ratio of renewable energy versus not renewable energy for the country (it is a change in the ratio since the logit link models odds, not percentages). Intuitively, this seems reasonable. Countries that signed the Paris Agreement later would like,ly be using a smaller percentage of renewable energy. Many of the other convention coefficients are non-zero and negative. For consumption related variables and mining, these are negative as well. This suggests that larger consumption of natural resources predicts less investment, which also makes sense. On the other hand, predicted areas (marine and terrestrial) are positive. This also makes sense, as countries that protect more of their land an resources may have a more environmental and sustainable outlook and be more invested in renewables energy.

Before using the model for prediction, it is worth briefly evaluating whether it is an appropriate model with some diagnostic checking (the diagnostic checking is relatively limited for this report, as the main purpose of this report prediction). Residual plots are shown below:

```{r beta_residuals, , class.source = 'fold-hide', echo = TRUE, results='show'}
# Model Residuals and Fitted Values
beta_model_residuals = residuals(beta_model, type = c("quantile"))
beta_model_fitted = fitted(beta_model)$mu[2]
beta_model_residuals_df = data.frame(res = beta_model_residuals,
                                     fit = beta_model_fitted)
colnames(beta_model_residuals_df) = c("res","fit")

# Histogram of Quantile Residuals
histogram_beta_residuals = ggplot(beta_model_residuals_df, aes(x = res)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(alpha = 0.1, fill = "red") +
  labs(x = "Quantile Residuals", y = "Density") +
  theme(text = element_text(size = 20),
    axis.text.y = element_text(angle=90, hjust=1, size = 10),
    axis.text.x = element_text(size = 10),
    plot.title = element_text(hjust = 0.5, size = 12),
    axis.title=element_text(size=10,face="bold")) +
  theme_minimal()

# QQ Plot of Quantile Residuals
qq_beta_residuals = qqplot_residuals(beta_model_residuals)

# Residual vs. Fitted Plot
resid_vs_fit_model = ggplot(beta_model_residuals_df, 
                              aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 2, color = "red", size = .5) +
  geom_smooth(formula = "y~x", method = "loess",
              se=TRUE, linetype = 2, color = "red", size = .25) +
  labs(x="Fitted Values (log-odds)",y="Residuals") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme_minimal()

# Plot
plot_grid(histogram_beta_residuals,qq_beta_residuals, resid_vs_fit_model,
          ncol = 2, nrow = 2, 
          labels = c("Histogram", "Quantile-Quantile Plot", "Residuals vs. Fitted Values"),
          label_size = 10,
          hjust = -.75)

```

[COME BACK AND DO OUTLIER PLOT IF TIME]

Quantile residuals are provided by the \textit{bamlss} package and are used here per the reccomendation from the bamlss website [CITE https://bayesr.r-forge.r-project.org/articles/glm.html]. The diagnostic plots overall suggest the beta regression model is an acceptable choice for the data, if not quite perfect. The histogram and the quantile-quantile plot (QQ Plot) both suggest the residuals are very roughly normally distributed. The histogram roughly looks like a normal distribution. The normal qunatiles and quantiles of the residuals roughly track, with some rightward skew. This suggests there are some outliers in the residuals on the right-hand side of the distribution (in the QQ Plot, the blue dots would track the black line exactly if the residuals were normally distributed). Finally, the residuals versus fitted plot shows the relationship between the residuals and the predicted values of the training data. Note the fitted values here are the log odds, and not in terms of percentages (as specified by the model). There should be now obvious relationship or heteroskedasticity. The plot suggests rough randomness in the relationship. Overall, the plots suggest that the model does not predict high renewable energy percentages particularly well, but the choice of a beta regression model seems somewhat justified.

Given the model is adequate, prediction is done using the test data. Note that predicted values are available for the posterior mean and variance parameters, but only the predicted value is analyzed here (not the uncertainty around said prediction).

```{r beta_prediction_error, , class.source = 'fold-show', echo = TRUE, results='show'}
# Use the predict function and the test data
beta.pred = predict(beta_model, newdata = x.test, type = "parameter")$mu

# Calculate Mean Squared Error and Mean Absolute Error
beta_rmse = sqrt(mean((y.test-beta.pred)^2))
beta_abs_e = mean(abs(y.test-beta.pred))

# Calculate Bias
beta.fit = predict(beta_model, newdata = x.train, type = "parameter")$mu
beta_bias = mean(beta.fit - y.train)
```

The mean squared error is .0752 and the mean absolute error is .2138. That means on average, the prediction was off by 21.38\%. As mentioned in the methods section, in the case of proportion data 33\% mean absolute error is the expected performance of an uninformed prediction. [NEED TO ADD THIS TO METHODS]. This implies that the sparse beta regre model does provide some help in prediction, but being off by 21.38\% on average is still fairly large.

```{r beta_prediction_graph, class.source = 'fold-hide', echo = TRUE, results='show'}
# Create Data Frame with Countries, Predicted Values
# And Actual Values
beta_predict_graph_data = data.frame(observation = c(1:39),
           prediction = beta.pred,
           actual = y.test,
           Country = x.test$Country_Final)

# Specifiy minimum and maxium in each case for plotting 
# the line between points
beta_predict_graph_data = beta_predict_graph_data %>%
  mutate(min_value_col = pmin(prediction,actual)) %>%
  mutate(max_value_col = pmax(prediction,actual)) %>%
  mutate(Abs_Diff = abs(actual-prediction))

# Plot the Predictions vs. Actual with difference lines and labels
ggplot(data = beta_predict_graph_data, aes(x = observation, label = Country)) + 
  geom_point(aes(y = prediction, color = 'Predicted')) +
  geom_point(aes(y = actual, color='Actual')) +
  geom_linerange(aes(ymin = min_value_col, ymax = max_value_col), 
                col = "blue", linetype = "dashed", alpha = .5) +
  geom_text(aes(y = actual), size = 2, vjust = -.5) +
  labs(title = "Test Dataset Results for Beta Regression Model",
       x="Country",y="Renewable Energy 2017 (%)") + 
  scale_color_manual(name='Renewable % 2017',
                     breaks=c('Actual', 'Predicted'),
                     values=c('Actual'='black', 'Predicted'='red')) +
  theme(legend.title=element_text(size=20), legend.text=element_text(size=14)) +
  theme_minimal()
```

The above graph shows the difference between the predicted values and the actual values for renewable energy percentage in 2017. Generally, the largest source of error appears to come from countries that have actual renewable percentages close to or greater than .7. 

```{r beta_prediction_analysis, class.source = 'fold-hide', echo = TRUE, results='show'}
# Create Dataframe with Absolute Difference
# Arrange to be in Order
beta_predict_analysis_1 = beta_predict_graph_data %>%
  dplyr::select(-c(min_value_col,max_value_col)) %>% 
  arrange(-Abs_Diff)

# Print out the Results
kable(rbind(head(beta_predict_analysis_1,5),
            tail(beta_predict_analysis_1,5)))
```

The above table confirms that the model performs worst at predicting high and 0 percentages. Interestingly, Palau was predicted to be the highest renewable percentage, but in fact is 0. Double checking the data, Palau having near zero renewable energy is confirmed by another source: the U.S. department of energy reported Palau's Electricity generation mix to be 97.5\% diesel in 2020 and 2.5\% solar [CITE https://www.energy.gov/sites/default/files/2020/09/f79/ETI-Energy-Snapshot-Palau_FY20.pdf]. Bhutan's high renewable percentage is due to its hydropower production that it both relies on for electricity and as a major export [NEED TO CITE THIS]. So while the model succeeds in predicting values between 0 and 1, it struggles to predict large percentages or percentages close to 0.

# Model 3: Random Forest

# Model 4: Boosted Trees

# Discussion of Results

# Conclusion