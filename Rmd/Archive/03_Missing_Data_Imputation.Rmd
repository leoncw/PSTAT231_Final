---
title: "03_Missing_Data_Imputation"
author: "Callum Weinberg"
date: "February 23, 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# This is necessary given the nested directories.
# Enter your working directory here
knitr::opts_knit$set(root.dir = "/home/clw/Documents/UCSB/Quarter6/PSTAT 231/Final Project/PSTAT231_Final")
```

# README

Missing data imputation. For now assigning median value. This will not be the final approach, but doing so to move along the analysis process

# Libraries

```{r libraries}
library(plyr)
library(dplyr)
library(tidyr)
library(assertr)
library(knitr)
library(ggplot2)
```

# Functions
```{r}
# Define Median Replacement function (for a matrix)
median_replace = function(x) {
  for(i in 1:ncol(x)) {
    # Note: this will break if an entire column is missing values
    x[which(is.na(x[,i])),i] = median(x[,i],na.rm=TRUE)
  }
  return(x)
}

# Test
y = matrix(c(1,2,3,NA,5,6,7,NA,NA), ncol = 3, byrow = FALSE)
y

g = median_replace(y)
g
```

# Load the Data

```{r load_data}
## Load the Data from the 01 File
load(file="Intermediate_Data/02_limited_data.Rdata")
```

# Impute Missing Data

Currently assigning Median value to missing data. This is not a good strategy, particularly for some of these variables (such as the years for international agreements). Working on a better method.

```{r impute_data}
# Replace missing values with the medians
#model_data = limited_data %>%
#  mutate(across(.cols = everything(), ~ifelse(is.na(.x) == TRUE, median(.x,na.rm = TRUE), .x)))
# DELETE THIS lm = lm(data = limited_data, renewable_percent_2017 ~ as.matrix(limited_data[,4:35]))
```

The goal here is to predict each missing value using a linear model of all other observations, where any missings in the fit for the model are substituted with the median (but only for the fitting and prediction stage).

```{r impute_data_updated}
# Count Number of Missings
colSums(is.na(limited_data))

# Define Model Data
model_data = limited_data

# Loop Over Each Variable Not in 
for(i in 7:ncol(limited_data)) {

  # Get Vector of Predictors (Column Position in limited_data)
  predictors = c(7:35)
  predictors = predictors[predictors != i]

  # Fit the Linear Model on All Precitors (excluding the)
  # variable that is being interpolated
  fit = 
    lm(data = limited_data, 
       as.matrix(limited_data[,i]) ~ as.matrix(median_replace(limited_data[,predictors])))
  
  # Predict
  prediction = predict(fit,as.data.frame(median_replace(limited_data[,predictors])))
  
  # No Predictions should be less than 0
  # This is based on the variables included, and
  # may change if more variables are included
  negative_indeces = which(prediction < 0)
  prediction[negative_indeces] = 0
  
  # Replace Data With Interpolated Data
  replace_indeces = which(is.na(model_data[,i]))
  model_data[replace_indeces,i] = prediction[replace_indeces]
  
  #model_data = model_data %>%
  #  mutate(Predict_Variable = prediction) %>%
  #  mutate(.[[i]] = ifelse(is.na(.[[i]]) == TRUE, Predict_Variable, .[[i]]))
}


# Create a Plot to Compare Data Sets before and After
distr_df = data.frame(Filled = model_data$World__Heritage_Convention, Missing = limited_data$World__Heritage_Convention) %>% 
  melt(value.name='y_i')

# Plot
ggplot(distr_df, aes(x = y_i, fill = variable)) +
  geom_histogram(position = "identity",binwidth = 1, alpha = .4) +
  labs(x = "y_i", y = "Frequency", 
       title = "Comparison") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 5)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 12))
```

# Output Data

```{r output_data}
# Save the Data for Use in the Next File
save(model_data, file="Intermediate_Data/03_model_data.Rdata")
```

