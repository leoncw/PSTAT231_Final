---
title: "Ridge_and_Lasso"
author: "Hailey Broderick"
date: "March 11, 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs}
library(ISLR)
library(glmnet)
library(dplyr)
library(tidyr)
library(tidymodels)
```

## Regularization Models

One model which we choose to build for our prediction is a linear regression model. Linear regression is one parametric method which we have learned. Since our response variable, renewable_percent_2017, is a quantitative variable, linear regression could be a good model for our data, under the following assumptions: there is a linear relationship between response and the predictors, errors of the model are i.i.d. normal random errors, and all observations follow this model. Since we have a relatively large number of predictors (30) relative to observations (195), the variance of our model may be quite large. Thus, to increase our prediction accuracy, we choose to use regularization methods in order to significantly reduce variance while only slightly increasing bias, leading to an overall decrease in MSE. In other words, regularization prevents overfitting. Regularization methods work by shrinking the coefficients of irrelevant variables for prediction accuracy towards zero. In selecting the most accurate linear regression model, we will preform both ridge regression and lasso regression. The main difference between these two methods is that ridge regularization can shrink estimates close to zero, whereas lasso regularization can shrink estimates to zero.

To create our regularized regression models, we use the function glmnet(). Generally we want to standardize data prior to regularization, but glmnet() automatically scales variables. The function takes the arguments alpha which determines fit, and lambda determines the penalty applied to the norm of the coefficients, resulting in different model coefficients (and in turn models) depending on the lambda parameter specified.

We begin by loading our data and subsetting it to exclude the variables Country_Final and Country_ID_Final, as the values of these variables are means of identifying observations, not values which should influence our model. Duplicate variable energy_supply_2017 is also excluded.
```{r model_data_load_subset}
# load original model data
load("~/Desktop/PSTAT231_Final/Intermediate_Data/03_model_data.Rdata")
# subset data removing Country_Final and Country_ID_Final, since these variables are simply a way of identifying country of observation, not a number with meaning, a model.matrix would dummy code these variables, but we do not want them dummy coded as they are separate identification variables.
subset_data = model_data %>%
  select(-c(Country_Final, Country_ID_Final, energy_supply_2017))
```


We use model.matrix() to create a design matrix of data with renewable_percent_2017 as the response. We will split this data into training and test sets, which we will use to fit and evaluate the performance of our final ridge and lasso regression models.
```{r design_matrix_regularization}
# construct design matrix with renewable_percent_2017 as our response, using all other variables as predictors
data.all <- model.matrix(renewable_percent_2017 ~ . , subset_data)
# seed set to reproduce original training and test sets created, but taken from a matrix (necessary to use glmnet())
set.seed(42)
# training set 80% of data
train_r = sample(nrow(data.all), .8*(nrow(data.all)))
x.train_r = data.all[train_r, ]
y.train_r = subset_data[train_r, ]$renewable_percent_2017
# test set remaining 20% of data
x.test_r = data.all[-train_r, ]
y.test_r = subset_data[-train_r, ]$renewable_percent_2017
```



### Ridge Regression Model

First we will apply the ridge regression regularization method in order to predict renewable_percent_2017 using all other variables as predictors.

To initialize ridge regularization, we use the function glmnet() with argument alpha=0 to specify a ridge regression model fit to our training data. We implement the function over a full range of possible lambda values, since this is the parameter which must be tuned to determine our optimal ridge model. We plot the standardized coefficients for ridge regression with respect to the range of possible lambdas.
```{r initial_ridge_model}
# vector of 100 possible lambda parameter values
lambda.list.ridge = 1000*exp(seq(0, log(1e-5), length = 100))
# Fit ridge regression model to training set, with lambda parameter left to tune
ridge.mod <- glmnet(x.train_r, y.train_r, alpha=0, lambda = lambda.list.ridge)
# plot standardized coefficients vs lambda values
plot(ridge.mod, xvar="lambda", label=TRUE)
```
We see that depending on the choice of penalty parameter lambda, coefficients of the ridge regression model are regularized towards zero. Our goal in creating the optimal ridge regression model is to find the optimal value of lambda which produces coefficients which minimize MSE.


We use cross validation to fold the training set into 10 folds (using k-fold CV function as defined in Homework 2). For each fold of the training data, we preform additional 5-fold CV via the cv.glmnet() to find the optimal value of lambda and resulting MSE for the corresponding validation sets. 
```{r CV_training_ridge}

set.seed(42)
train = sample(nrow(subset_data), .8*(nrow(subset_data)))
x.train = subset_data[train, ]
y.train = subset_data[train, ]$renewable_percent_2017

data.train <- model.matrix(renewable_percent_2017 ~ . , x.train)


# k-fold CV function
do.chunk.ridge <- function(chunkid, folddef, dat, ...){
  # get training index
  train_id = (folddef != chunkid)
  # training set
  dat.train.x = dat[train_id, ]
  dat.train.y = x.train[train_id, ]$renewable_percent_2017
  # validation set
  dat.val.x = dat[-train_id, ]
  dat.val.y = x.train[-train_id, ]$renewable_percent_2017
  # vector possible lambda values
  lambda.list.ridge = 1000  * exp(seq(0, log(1e-5), length = 100))
  # train ridge regression model on training data
  fit.ridge.train <- glmnet(dat.train.x, dat.train.y, alpha=0, lambda = lambda.list.ridge)
  # 5-fold CV for optimal lambda
  cv.out.ridge = cv.glmnet(dat.train.x, dat.train.y, alpha=0, folds=5)
  bestlam.r = cv.out.ridge$lambda.min
  # get predicted value on validation set
  pred.ridge.val = predict(fit.ridge.train, s=bestlam.r, newx=dat.val.x)
  # create data frame of validation errors
  data.frame(fold = chunkid,
             bestlam.r = cv.out.ridge$lambda.min,
             val.MSE = mean((pred.ridge.val - dat.val.y)^2),
             val.RMSE = sqrt(mean((pred.ridge.val - dat.val.y)^2)),
             val.MAE = mean(abs(dat.val.y-pred.ridge.val)))
}


set.seed(42)
# set k-fold to 10
nfold=10
# create folds
folds = cut(1:nrow(x.train), breaks=nfold, labels=FALSE) %>% sample()
# set vector to save validation error
error.folds.r = NULL
# perform 10-fold CV
for (i in 1:10){
  CV.r = do.chunk.ridge(chunkid=i, folddef=folds, dat=data.train)
  error.folds.r = rbind(error.folds.r, CV.r)
}

error.folds.r
optimal_lam_r1 = error.folds.r[2,]$bestlam.r
```
Based on the 10 folds of our training data, we see training fold 2 resulted in lowest validation MSE, indicating it's corresponding lambda = `r optimal_lam_r1` may be our optimal penalty parameter.



We compare this lambda value selected from the folded training set, to the optimal lambda chosen by preforming 10-fold cross-validation on the entire training set, simply using the function cv.glment(). The plot of lambda values and corresponding MSEs is produced.
```{r optimal_lambda_ridge}
set.seed(42)
# 5-fold CV
cv.out.ridge = cv.glmnet(x.train_r, y.train_r, alpha=0, folds=10)
plot(cv.out.ridge)
abline(v = log(cv.out.ridge$lambda.min), col="green4", lwd=3, lty=2)
# optimal lambda
optimal_lam_r2 = cv.out.ridge$lambda.min
```
According to 10-fold CV preformed on the entire training set, the optimal value of tuning parameter lambda for our ridge regression model is `r optimal_lam_r2`.


In order to choose which lambda value to use for our optimal ridge regression model, we fit two ridge regression models to our original training set for both values of lambda. 
- Fit one is ridge.mod1 with lambda =`r optimal_lam_r1`
- Fit two is ridge.mod2 with lambda = `r optimal_lam_r2`
```{r compare_models_ridge}
# fit ridge model 1
ridge.mod1 <- glmnet(x.train_r, y.train_r, alpha=0, lambda = optimal_lam_r1)
ridge.train.pred1 = predict(ridge.mod1, s=optimal_lam_r1, newx=x.train_r)
ridge.train.MSE1 = mean((ridge.train.pred1- y.train_r)^2)
# fir ridge model 2
ridge.mod2 <- glmnet(x.train_r, y.train_r, alpha=0, lambda = optimal_lam_r2)
ridge.train.pred2 = predict(ridge.mod2, s=optimal_lam_r2, newx=x.train_r)
ridge.train.MSE2 = mean((ridge.train.pred2- y.train_r)^2)
```
Based on the predictions of the two ridge models, we see that ridge.mod1 with lambda=`r optimal_lam_r1` results in a slightly lower training MSE `r ridge.train.MSE1`, compared to training MSE `r ridge.train.MSE2` from ridge.mod2 with lambda=`r optimal_lam_r2`. This means that ridge.mod1 is slightly better fit to the training data.


Thus, we choose tuning parameter `r optimal_lam_r1` to be the optimal lambda value to build our final ridge regression model.

The coefficients for our optimally tuned ridge regression model are the following:
```{r ridge_coeff}
ridge.coeff = predict(ridge.mod1, type="coefficients", s=optimal_lam_r1)
ridge.coeff
```
Our final ridge regression model has 30 coefficient estimates, some of these coefficient estimates appear to be very close to zero, but none are exactly zero.

We now compute the training MSE and test MSE for our optimal ridge regression model:
```{r ridge_MSE}
ridge.train.pred = predict(ridge.mod1, s=optimal_lam_r1, newx=x.train_r)
ridge.train.MSE = mean((ridge.train.pred - y.train_r)^2)

ridge.test.pred = predict(ridge.mod1, s=optimal_lam_r1, newx=x.test_r)
ridge.test.MSE = mean((ridge.test.pred - y.test)^2)
```
Our final ridge regression model based on the coefficient estimates produced by our optimally tuned lambda results in training MSE `r ridge.train.MSE`, and test MSE `r ridge.test.MSE`. The test MSE is lower than the training MSE, although both are relatively close to zero. This is promising and signals that our model preforms well on the test set.




### Lasso Regression Model

We will now apply the lasso regularization method in order to predict renewable_percent_2017 using all other variables as predictors.

To begin lasso regularization, we proceed in the same manner as ridge regularization, using the function glmnet() implemented over a full range of possible lambda parameters. The only difference occurs for the argument alpha, which we must set to alpha=1 to specify a lasso regression model. We plot the standardized coefficients versus the range of possible lambda values.
```{r initial_lasso_model}
set.seed(42)
# vector of 100 possible lambda parameter values
lambda.list.lasso = 2 * exp(seq(0, log(1e-4), length=100))
# fit lasso regression model to training set, with lambda parameter left to tune
lasso.mod <- glmnet(x.train_r, y.train_r, alpha=1, lambda=lambda.list.lasso)
# plot standardized coefficients vs lambda values
plot(lasso.mod, xvar="lambda", label=TRUE)
```
We see that as penalty parameter lambda increases, normalized coefficients of the lasso regression model are regularized down towards zero, eventually all becoming zero. Our goal in creating the optimal lasso regression model is to find the optimal value of lambda which produces coefficients that minimize prediction error.


Again, we use cross validation to fold the training set into 10 folds as we did for ridge regression. For each fold of the training data, we preform additional 5-fold CV via the cv.glmnet() to find the optimal value of lambda and resulting MSE for the corresponding validation sets. 
```{r CV_training_lasso}

set.seed(42)
train = sample(nrow(subset_data), .8*(nrow(subset_data)))
x.train = subset_data[train, ]
y.train = subset_data[train, ]$renewable_percent_2017

# design matrix of training data with response variable renewable_percent_2017
data.train <- model.matrix(renewable_percent_2017 ~ . , x.train)

# k-fold CV function
do.chunk.lasso <- function(chunkid, folddef, dat, ...){
  # get training index
  train_id = (folddef != chunkid)
  # training set
  dat.train.x = dat[train_id, ]
  dat.train.y = x.train[train_id, ]$renewable_percent_2017
  # validation set
  dat.val.x = dat[-train_id, ]
  dat.val.y = x.train[-train_id, ]$renewable_percent_2017
  # vector possible lambda values
  lambda.list.lasso = 2 * exp(seq(0, log(1e-4), length=100))
  # train ridge regression model on training data
  fit.lasso.train <- glmnet(dat.train.x, dat.train.y, alpha=1, lambda = lambda.list.lasso)
  # 5-fold CV for optimal lambda
  cv.out.lasso = cv.glmnet(dat.train.x, dat.train.y, alpha=1, folds=5)
  bestlam.l = cv.out.lasso$lambda.min
  # get predicted value on validation set
  pred.lasso.val = predict(fit.lasso.train, s=bestlam.l, newx=dat.val.x)
  # create data frame of validation errors
  data.frame(fold = chunkid,
             bestlam.l = cv.out.lasso$lambda.min,
             val.MSE = mean((pred.lasso.val - dat.val.y)^2),
             val.RMSE = sqrt(mean((pred.lasso.val - dat.val.y)^2)),
             val.MAE = mean(abs(dat.val.y-pred.lasso.val)))
}


set.seed(42)
# set k-fold to 10
nfold=10
# create folds
folds = cut(1:nrow(x.train), breaks=nfold, labels=FALSE) %>% sample()
# set vector to save validation error
error.folds.l = NULL
# perform 10-fold CV
for (i in 1:10){
  CV.l = do.chunk.lasso(chunkid=i, folddef=folds, dat=data.train)
  error.folds.l = rbind(error.folds.l, CV.l)
}

error.folds.l
optimal_lam_l1 = error.folds.l[4,]$bestlam.l
```

Based on the 10 folds of our training data, we see training fold 4 resulted in lowest validation MSE, indicating it's corresponding lambda = `r optimal_lam_l1` may be our optimal penalty parameter.


We compare this lambda value selected from the folded training sets, to the optimal lambda chosen by preforming 10-fold cross-validation on the entire training set, simply using the function cv.glment(). The plot of lambda values and corresponding MSEs is produced.
```{r optimal_lambda_lasso}
set.seed(42)
# 10-fold CV on training set
cv.out.lasso = cv.glmnet(x.train_r, y.train_r, alpha=1, folds=10)
plot(cv.out.lasso)
abline(v = log(cv.out.lasso$lambda.min), col="green4", lwd=3, lty=2)
# optimal lambda
optimal_lam_l2 = cv.out.lasso$lambda.min
```
The optimal lambda parameter selected for our lasso model by preforming 10-fold CV on our entire training set is `r optimal_lam_l2`.


In order to choose which tuning parameter lambda to use for our optimal lasso regression model, we fit two lasso models to our original training set for both values of lambda. 
- Fit one is lasso.mod1 with lambda =`r optimal_lam_l1`.
- Fit two is lasso.mod2 with lambda = `r optimal_lam_l2`.
```{r compare_models_lasso}
# fit ridge model 1
lasso.mod1 <- glmnet(x.train_r, y.train_r, alpha=1, lambda = optimal_lam_l1)
lasso.train.pred1 = predict(lasso.mod1, s=optimal_lam_l1, newx=x.train_r)
lasso.train.MSE1 = mean((lasso.train.pred1- y.train_r)^2)
# fir ridge model 2
lasso.mod2 <- glmnet(x.train_r, y.train_r, alpha=1, lambda = optimal_lam_l2)
lasso.train.pred2 = predict(lasso.mod2, s=optimal_lam_l2, newx=x.train_r)
lasso.train.MSE2 = mean((lasso.train.pred2- y.train_r)^2)
```
Based on the predictions of the two lasso models, we see that lasso.mod1 with lambda=`r optimal_lam_l1` results in a slightly lower training MSE `r lasso.train.MSE1`, compared to training MSE `r lasso.train.MSE2` from lasso.mod2 with lambda=`r optimal_lam_l2`. This means that lasso.mod1 is slightly better fit to the training data.

Thus, we choose tuning parameter `r optimal_lam_l1` to be the optimal lambda value to build our final lasso regression model.

The coefficients for our optimally tuned lasso regression model are the following:
```{r lasso_coeff}
lasso.coeff = predict(lasso.mod1, type="coefficients", s=optimal_lam_l1)
lasso.coeff
```
Our final lasso model has 19 out of 30 coefficients set to zero. The lasso model is more sparse compared to the ridge regression model, this could be advantageous in our prediction and analysis. 


We now compute the training MSE and test MSE for our optimal lasso regression model:
```{r lasso_MSE}
# training MSE
lasso.train.pred = predict(lasso.mod1, s=optimal_lam_l1, newx=x.train_r)
lasso.train.MSE = mean((lasso.train.pred - y.train_r)^2)
# test MSE
lasso.test.pred = predict(lasso.mod1, s=optimal_lam_l1, newx=x.test_r)
lasso.test.MSE = mean((lasso.test.pred - y.test)^2)
```
Our final lasso model results in training MSE `r lasso.train.MSE`, and test MSE `r lasso.test.MSE`. The training MSE is higher than the test MSE for our lasso model, meaning that our model preforms better on the test data than the data which it was fit to, which is positive because it signals that our lasso model is not overfit to the training data.



### Optimal Regression Model

Now that we have selected two final regression models found via regularization methods, ridge and lasso, we compare the models to choose our optimal linear regression model.

Compute test mean square errors:
```{r optimal_regression_MSE}
# mse for ridge model on test data
ridge.test.pred = predict(ridge.mod1, s=optimal_lam_r1, newx=x.test_r)
MSE.r = mean((ridge.test.pred - y.test)^2)
# mse for lasso model on test data
lasso.test.pred = predict(lasso.mod1, s=optimal_lam_l1, newx=x.test_r)
MSE.l = mean((lasso.test.pred - y.test)^2)
```
Compute test root mean square errors:
```{r optimal_regression_RMSE}
# root mse for ridge model on test data
RMSE.r = sqrt(ridge.test.MSE)
# root mse for lasso model on test data
RMSE.l = sqrt(lasso.test.MSE)
```
Compute test root mean absolute errors:
```{r optimal_regression_MAE}
# mean absolute error for ridge model on test data
MAE.r = mean(abs(y.test-ridge.test.pred))
# mean absolute error for lasso model on test data
MAE.l = mean(abs(y.test-lasso.test.pred))
```
Based on our evaluations of different metrics of accuracy, we see that both our final ridge and lasso regression models preform very closely on the test data. Our ridge model preforms slightly better on the test set measured on all metrics of accuracy. 

-Our final ridge model has test errors: MSE `r MSE.r`, RMSE `r RMSE.r`, and MAE `r MAE.r`
-Our final lasso model has test errors: MSE `r MSE.l`, RMSE `r RMSE.l`, and MAE `r MAE.l`

Since our lasso model uses significantly less predictors in its model (11 compared to 30), and has test MAE almost identical compared to the ridge model, it would be advantageous to use the lasso model in this setting, since we gain interpretability with much loss of accuracy. Thus, our optimal linear regression model chosen via regularization methods is the lasso model.

Finally, we plot the predictions for our lasso model on the test set versus the actual values in order to visualize how well our model preformed. The code used to produce this plot was created by Callum Weinberg, and slightly altered for our lasso model.
```{r lasso_model_prediction_graph}
lasso.test.pred = predict(lasso.mod1, s=optimal_lam_l1, newx=x.test_r)
# Create Data Frame with Countries, Predicted Values
# And Actual Values
lasso_predict_graph_data = data.frame(observation = c(1:39),
                                      s1 = lasso.test.pred,
                                      actual = y.test,
                                      Country = x.test$Country_Final)
# Specifiy minimum and maxium in each case for plotting 
# the line between points
lasso_predict_graph_data = lasso_predict_graph_data %>%
  mutate(min_value_col = pmin(s1,actual)) %>%
  mutate(max_value_col = pmax(s1,actual)) %>%
  mutate(Abs_Diff = abs(actual-s1))
# Plot the Predictions vs. Actual with difference lines and labels
ggplot(data = lasso_predict_graph_data, aes(x = observation, label = Country)) + 
  geom_point(aes(y = s1, color = 'Predicted')) +
  geom_point(aes(y = actual, color='Actual')) +
  geom_linerange(aes(ymin = min_value_col, ymax = max_value_col), 
                col = "blue", linetype = "dashed", alpha = .5) +
  geom_text(aes(y = actual), size = 2, vjust = -.5) +
  labs(title = "Test Dataset Results for Lasso Regression Model",
       x="Country",y="Renewable Energy 2017 (%)") + 
  scale_color_manual(name='Renewable % 2017',
                     breaks=c('Actual', 'Predicted'),
                     values=c('Actual'='black', 'Predicted'='red')) +
  theme(legend.title=element_text(size=20), legend.text=element_text(size=14)) +
  theme_minimal()
```
It is clear that our lasso model struggles to predict renewable energy percentage for countries which have relatively high renewable energy percentages (above 50%) as well as countries with low renewable energy percentages close to zero. Our lasso models' prediction values primarily fall between 10% and 50%, which poses a challenge in prediction since we hope to accurately predict the full range of renewable percentages.
