---
title: "Random_Forest"
author: "Hailey Broderick"
date: "March 11, 2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs}
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(dplyr)
```

```{r load_data}
# load original model data
load("~/Desktop/PSTAT231_Final/Intermediate_Data/03_model_data.Rdata")
```

```{r subset_split}
# subset original data excluding variables which are not to be included in or random forest 
set.seed(42)
subset_data = model_data %>%
  select(-c(Country_Final, Country_ID_Final, energy_supply_2017))
# training set 80% of original data
train = sample(nrow(subset_data), .8*(nrow(subset_data)))
x.train_rf = subset_data[train, ]
y.train_rf = subset_data[train, ]$renewable_percent_2017
# test set remaining 20% of original data
x.test_rf = subset_data[-train, ]
y.test_rf = subset_data[-train, ]$renewable_percent_2017
```


### Random Forest Regression Model


Now, we apply random forests to our data in order to predict renewable_percent_2017. In doing so, we will investigate how the features of our data set influence the renewable energy percentage generated by a Country. Since our response variable, renewable_percent_2017 is a continuous variable, we will build a Random Forest Regression Model. The randomForest package will be used to produce our Random Forest Model. Random Forests are beneficial to use over single decision trees, because they aggregate many deep trees (resulting in lower variance for error), in order to avoiding overfitting which tends to occur for single decision trees. The random forest method creates a plethora of decision trees fit on the training set, with a subset of different predictors used for the trees creating the random forest. The random forest makes predictions by averaging the decisions of the many individual trees which make up the random forest.

```{r rf_design_matrix}
# construct design matrix with renewable_percent_2017 as our response, using all other variables as predictors
data.all <- model.matrix(renewable_percent_2017 ~ . , subset_data)
# seed set to reproduce original training and test sets created, but taken from a matrix (necessary to use glmnet())
set.seed(42)
# training set 80% of data
train_r = sample(nrow(data.all), .8*(nrow(data.all)))
x.train_r = data.all[train_r, ]
y.train_r = subset_data[train_r, ]$renewable_percent_2017
# test set remaining 20% of data
x.test_r = data.all[-train_r, ]
y.test_r = subset_data[-train_r, ]$renewable_percent_2017
```



To build a random forest of regression trees, the number of predictors that should be considered for each split is $p/3$, where $p$ is the number of predictors ($p/3$ the argument used to specify regression trees). In our case, the data we use for the random forest has $p=30$ predictors, thus  $30/3 = 10$ predictors should be considered for each split of the tree. This parameter is specified in our model by mtry=10. The number of trees grown by randomForest() is specified by the argument ntree (by default for our model ntree=500). 
```{r rf_regression}
set.seed(42)
RF.regression <- randomForest(renewable_percent_2017 ~ . , data=x.train_rf, mtry=10, importance=TRUE)
plot(RF.regression)
```
500 trees is reasonable (and default) as we see that the MSE of the random forest model is relatively steady near zero after approximately 200 trees. 


Although cross-validation is inherently contained in the random forest method itself, we preform 5-fold cv on the training set to show an example of how the number of variables included selected for each tree affects the error of our folded training data.
```{r CV_training_ridge}
# Set Seed for Reproducibility
set.seed(42)
# design matrix necessary to use rfcv() random forest cross-validation function
# https://stackoverflow.com/questions/19760169/how-to-perform-random-forest-cross-validation-in-r
RF.cv <- rfcv(x.train_r, y.train_r, cv.fold=5)
with(RF.cv, plot(n.var, error.cv))
```
Cross-validation preformed on the training data reveals that varying the number of predictors selected for each split in our random forest model influences the error of the respective random forest models. The plot confirms that prediction errors should be smallest around 10 predictors used for each split. mtry=10 is the number of predictors which we specify in our random forest model, which signifies regression for random forests.


We will use our random forest model RF.regression with mtry=10 and ntree=500 as our final random forest model.
Look at the order of important variables for our random forest regression model:
```{r rf_regression_importance, fig.height=8, fig.width=10}
importance(RF.regression)
varImpPlot(RF.regression, n.var=20)
```
Based on the plot of the importance of variables in our random forests decision making, we note that forest_area_2015, CO2, protected_areas_marine_terrestrial, and NOx are among the top most important variables. This is decided because when these predictors are not included in decision trees, MSE is increased by over 6%. This points to emissions variables and land varibales being the most important predictors

Our random forest model is fit to both training and test sets in order to calculate the error of their predictions.
```{r mse_rf}
# training MSE
RF.train.pred = predict(RF.regression, newdata=x.train_r)
RF.train.MSE = mean((RF.train.pred - y.train)^2)
<<<<<<< HEAD
# test MSE
RF.test.pred = predict(RF.regression, newdata=x.test_r)
RF.test.MSE = mean((RF.test.pred - y.test)^2)
=======

# test MSE
RF.test.pred = predict(RF.regression, newdata=x.test_r)
RF.test.MSE = mean((RF.test.pred - y.test)^2)

>>>>>>> ba5f7f4bb7951dbee9012fd740a317503212bd65
```
Thus, our random forest regression model is has training MSE `r RF.train.MSE`, and test MSE `r RF.test.MSE`. Thus, our random forest regression model is better fit to the training data than the test data. Both training and test MSE are relatively close to zero, which means that the model has some prediction accuracy on the test set.


We now calculate different metrics of accuracy in order to compare our random forest regression model's performance with other models considered throughout the report.

Compute and compare test MSE, RMSE, and MAE:
```{r MSE}
# test mse for RF  model on test data
RF.test.pred = predict(RF.regression, newdata=x.test_r)
RF.test.MSE = mean((RF.test.pred - y.test)^2)
<<<<<<< HEAD
# test root mse 
RF.test.RMSE = sqrt(RF.test.MSE)
# mean absolute error for RF on test data
RF.MAE = mean(abs(y.test_r-RF.test.pred))
=======
RF.test.MSE
# test root mse 
RF.test.RMSE = sqrt(RF.test.MSE)
RF.test.RMSE
# mean absolute error for RF on test data
RF.MAE = mean(abs(y.test_r-RF.test.pred))
RF.MAE
>>>>>>> ba5f7f4bb7951dbee9012fd740a317503212bd65
```
The resulting test errors made by our RF model are: MSE = `r RF.test.MSE` , RMSE = `r RF.test.RMSE` , and MAE = `r RF.MAE`.

The mean absolute error of our random forest regression model is `r RF.MAE` which is an okay mean absolute error. This type of model does not seem to be ideal for our prediction needs, but it may be able to predict with some accuracy.

Finally, we plot the predictions for our random forest model on the test set versus the actual values in order to visualize how well our model preformed. The code used to produce this plot was created by Callum Weinberg, and slightly altered for our random forest model.
```{r lasso_model_prediction_graph}
set.seed(42)
RF.test.pred = as.matrix(predict(RF.regression, newdata=x.test_r))
RF.test.pred
# Create Data Frame with Countries, Predicted Values
# And Actual Values
RF.test.pred
RF_predict_graph_data = data.frame(observation = c(1:39),
                                      V1 = RF.test.pred,
                                      actual = y.test,
                                      Country = x.test$Country_Final)
# Specifiy minimum and maxium in each case for plotting 
# the line between points
RF_predict_graph_data = RF_predict_graph_data %>%
  mutate(min_value_col = pmin(V1,actual)) %>%
  mutate(max_value_col = pmax(V1,actual)) %>%
  mutate(Abs_Diff = abs(actual-V1))
# Plot the Predictions vs. Actual with difference lines and labels
ggplot(data = RF_predict_graph_data, aes(x = observation, label = Country)) + 
  geom_point(aes(y = V1, color = 'Predicted')) +
  geom_point(aes(y = actual, color='Actual')) +
  geom_linerange(aes(ymin = min_value_col, ymax = max_value_col), 
                col = "blue", linetype = "dashed", alpha = .5) +
  geom_text(aes(y = actual), size = 2, vjust = -.5) +
  labs(title = "Test Dataset Results for Random Forest Regression Model",
       x="Country",y="Renewable Energy 2017 (%)") + 
  scale_color_manual(name='Renewable % 2017',
                     breaks=c('Actual', 'Predicted'),
                     values=c('Actual'='black', 'Predicted'='red')) +
  theme(legend.title=element_text(size=20), legend.text=element_text(size=14)) +
  theme_minimal()
```
<<<<<<< HEAD
Our random forest regression model has difficulty predicting renewable energy percentage for countries which have relatively high renewable energy percentages (above 50%), but is sightly better at predicting countries with low renewable energy percentages close to zero. Our random forest model only predicts values renewable energy percentages under 50%, this raises concerns about using a random forest model to best predict our data. It may be difficult to predict high renewable energy percentages due to fewer observations of high renewable energy percentage countries in our model data (skewed distribution).
=======
Our random forest regression model has difficulty predicting renewable energy percentage for countries which have relatively high renewable energy percentages (above 50%), but is sightly better at predicting countries with low renewable energy percentages close to zero. Our random forest model only predicts values renewable energy percenteges under 50%, this raises concerns about using a random forest model to best predict our data. It may be difficult to predict high renewable energy percentages due to fewer observations of high renewable energy percentage countries in our model data (skewed distribution).
>>>>>>> ba5f7f4bb7951dbee9012fd740a317503212bd65


